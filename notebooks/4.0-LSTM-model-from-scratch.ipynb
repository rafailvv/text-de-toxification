{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "0KfMI0srA5Rs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, vocab=None, max_length=None):\n",
        "        self.data = data\n",
        "        self.vocab = vocab or self.build_vocab()\n",
        "        self.max_length = max_length or self.get_max_length()\n",
        "        self.data['reference_int'] = self.data['reference'].apply(self.text_to_ints)\n",
        "        self.data['translation_int'] = self.data['translation'].apply(self.text_to_ints)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        reference_int = self.pad_features(self.data.iloc[idx]['reference_int'])\n",
        "        translation_int = self.pad_features(self.data.iloc[idx]['translation_int'])\n",
        "        return (\n",
        "            torch.tensor(reference_int, dtype=torch.long),\n",
        "            torch.tensor(translation_int, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def build_vocab(self):\n",
        "        word_counts = Counter()\n",
        "        for _, row in self.data.iterrows():\n",
        "            word_counts.update(row['reference'].split())\n",
        "            word_counts.update(row['translation'].split())\n",
        "        return {word: i+1 for i, (word, _) in enumerate(word_counts.most_common())}\n",
        "\n",
        "    def get_max_length(self):\n",
        "        return max(\n",
        "            self.data['reference'].apply(lambda x: len(x.split())).max(),\n",
        "            self.data['translation'].apply(lambda x: len(x.split())).max()\n",
        "        )\n",
        "\n",
        "    def text_to_ints(self, text):\n",
        "        return [self.vocab.get(word, 0) for word in text.split()]\n",
        "\n",
        "    def pad_features(self, text_ints):\n",
        "        return text_ints + [0] * (self.max_length - len(text_ints))"
      ],
      "metadata": {
        "id": "j1Zqc-pGAxXm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# LSTM Model Class\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=0.5, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.linear(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.lstm.num_layers, sequence_length, self.lstm.hidden_size),\n",
        "                torch.zeros(self.lstm.num_layers, sequence_length, self.lstm.hidden_size))\n"
      ],
      "metadata": {
        "id": "64Xajv3NA1AL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, criterion, optimizer, num_epochs, vocab_size):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(total=len(data_loader), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "\n",
        "        # Initial hidden state\n",
        "        state_h, state_c = model.init_state(data_loader.batch_size)\n",
        "\n",
        "        for batch, (x, y) in enumerate(data_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # If the last batch is smaller, reinitialize the state with the correct batch size\n",
        "            if x.size(0) != data_loader.batch_size:\n",
        "                state_h, state_c = model.init_state(x.size(0))\n",
        "\n",
        "            # Detach the states from the history of the last batch\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.update()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        progress_bar.close()\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loader):.4f}')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "vQmiQ4u8Bmpq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(data_loader, model, vocab):\n",
        "    bleu_scores = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in data_loader:\n",
        "            predicted, _ = model(x, model.init_state(1))\n",
        "            predicted = torch.argmax(predicted, dim=2)\n",
        "            pred_list = predicted.numpy().tolist()\n",
        "            real_list = y.numpy().tolist()\n",
        "\n",
        "            # Convert integer sequences to words\n",
        "            pred_words = [[vocab[i] for i in seq if i != 0] for seq in pred_list]\n",
        "            real_words = [[[vocab[i] for i in seq if i != 0]] for seq in real_list]\n",
        "\n",
        "            # Calculate BLEU score\n",
        "            bleu_scores.append(corpus_bleu(real_words, pred_words, smoothing_function=SmoothingFunction().method1))\n",
        "\n",
        "    return np.mean(bleu_scores)"
      ],
      "metadata": {
        "id": "JNo1THQxBwlF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    x, y = zip(*batch)\n",
        "    x_padded = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in x],\n",
        "                            batch_first=True, padding_value=0)\n",
        "    y_padded = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in y],\n",
        "                            batch_first=True, padding_value=0)\n",
        "    return x_padded, y_padded"
      ],
      "metadata": {
        "id": "qq81hGVrKk9P"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "train_data = train_data.head(1000)\n",
        "test_data = test_data.head(1000)\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = TextDataset(train_data)\n",
        "test_dataset = TextDataset(test_data, vocab=train_dataset.vocab, max_length=train_dataset.max_length)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "W86Xx2SnBygD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = LSTMModel(\n",
        "    vocab_size=len(train_dataset.vocab) + 1,\n",
        "    embedding_dim=256,\n",
        "    hidden_dim=512,\n",
        "    n_layers=2\n",
        ")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "pqkHpRQ2B1w5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = train(model, train_loader, criterion, optimizer, num_epochs=10, vocab_size=len(train_dataset.vocab) + 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_KTjBa-B27X",
        "outputId": "6fe300ae-6196-432c-bcd2-ce9bcd64d6da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 2.5227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 1.5274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 1.4512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 1.4361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 1.4008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 1.4008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 1.3725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 1.3296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 1.3017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 1.2838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert integers to text\n",
        "def ints_to_text(ints, vocab):\n",
        "    return ' '.join([vocab[i] for i in ints if i > 0])  # assuming 0 is the padding value\n",
        "\n",
        "# Prediction function\n",
        "def predict(model, sentence, vocab, max_length):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Convert sentence to integers\n",
        "        text_ints = [vocab.get(word, 0) for word in sentence.split()]\n",
        "\n",
        "        # Pad sequence\n",
        "        text_ints_padded = text_ints + [0] * (max_length - len(text_ints))\n",
        "        text_tensor = torch.tensor(text_ints_padded, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        # Predict\n",
        "        logits, _ = model(text_tensor, model.init_state(1))\n",
        "        prediction_ints = torch.argmax(logits, dim=2).squeeze(0).tolist()\n",
        "\n",
        "        # Convert integers back to text\n",
        "        predicted_sentence = ints_to_text(prediction_ints, {i: word for word, i in vocab.items()})\n",
        "\n",
        "        return predicted_sentence\n"
      ],
      "metadata": {
        "id": "kXLX837bEDMp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text to de-toxify\n",
        "example_text = \"I like that shit.\"\n",
        "\n",
        "# Predict a less toxic version\n",
        "less_toxic_version = predict(model, example_text, train_dataset.vocab, train_dataset.max_length)\n",
        "print(f\"Original: {example_text}\")\n",
        "print(f\"De-toxified: {less_toxic_version}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17gf6nozEFO6",
        "outputId": "317b5e2d-f0ce-403f-f11c-aa67246300fc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: I like that shit.\n",
            "De-toxified: I you you .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text to de-toxify\n",
        "example_text = \"It told you this was a waste of my fucking time.\"\n",
        "\n",
        "# Predict a less toxic version\n",
        "less_toxic_version = predict(model, example_text, train_dataset.vocab, train_dataset.max_length)\n",
        "print(f\"Original: {example_text}\")\n",
        "print(f\"De-toxified: {less_toxic_version}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZvfcBMqGjtC",
        "outputId": "2ebef0de-ac84-4a99-80b5-5b5758f4fd1d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: It told you this was a waste of my fucking time.\n",
            "De-toxified: you you to a . ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text to de-toxify\n",
        "example_text = \"Funny how Nazis are always the bad guys.\"\n",
        "\n",
        "# Predict a less toxic version\n",
        "less_toxic_version = predict(model, example_text, train_dataset.vocab, train_dataset.max_length)\n",
        "print(f\"Original: {example_text}\")\n",
        "print(f\"De-toxified: {less_toxic_version}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcwXUCCDNOgl",
        "outputId": "79fadb55-512e-41c2-c78d-889661de089a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Funny how Nazis are always the bad guys.\n",
            "De-toxified: I 's to to the the .\n"
          ]
        }
      ]
    }
  ]
}