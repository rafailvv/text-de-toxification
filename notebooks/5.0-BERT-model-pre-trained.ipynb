{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki_ybmkQlIz7",
        "outputId": "206ce62f-94fb-4778-b301-d6217cb29b5b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers[torch])\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, accelerate, transformers\n",
            "Successfully installed accelerate-0.24.1 huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c8mVO-kDWvIH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('most_toxic_data.csv').head(1000)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokens = tokenizer(df['reference'].tolist(), max_length=128, padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "2uouydkqnALc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class DetoxificationDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "dataset = DetoxificationDataset(tokens, df['trn_tox'])\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "ynwYbdBTlWGu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "epochs = 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1qU1iuym3jQ",
        "outputId": "dfe8ce5d-816e-40f3-a2a3-016bf3349641"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'epochs' is defined and you have a 'dataloader' for your data\n",
        "loss_values = []  # List to store loss values\n",
        "\n",
        "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "    total_loss = 0  # Variable to store the epoch loss\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Iteration\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Unpack the batch contents and feed them into the model\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        # Forward pass and compute loss\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss over the entire epoch\n",
        "    avg_epoch_loss = total_loss / len(dataloader)\n",
        "    print(f\"Average Loss at Epoch {epoch}: {avg_epoch_loss}\")\n",
        "\n",
        "    # Store the average loss value for plotting or analysis later\n",
        "    loss_values.append(avg_epoch_loss)\n",
        "\n",
        "# After training loop, you might want to save the model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGT-7RjdlX6n",
        "outputId": "f7e5f284-dd2e-483c-baa2-8ce5f63a79d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A<ipython-input-4-a53a1f3cd555>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "\n",
            "Iteration:   2%|▏         | 1/63 [00:17<17:42, 17.14s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/63 [00:26<12:48, 12.59s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/63 [00:37<11:36, 11.62s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/63 [00:46<10:28, 10.66s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/63 [00:55<09:52, 10.22s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/63 [01:06<09:45, 10.28s/it]\u001b[A\n",
            "Iteration:  11%|█         | 7/63 [01:14<09:06,  9.76s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/63 [01:24<08:58,  9.79s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 9/63 [01:35<09:02, 10.05s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/63 [01:44<08:38,  9.78s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 11/63 [01:54<08:30,  9.82s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/63 [02:04<08:29, 10.00s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/63 [02:13<07:57,  9.54s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 14/63 [02:23<08:06,  9.92s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/63 [02:34<08:02, 10.06s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 16/63 [02:42<07:30,  9.58s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/63 [02:53<07:31,  9.82s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 18/63 [03:03<07:29,  9.99s/it]\u001b[A\n",
            "Iteration:  30%|███       | 19/63 [03:12<06:59,  9.54s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/63 [03:22<07:00,  9.79s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 21/63 [03:33<07:04, 10.11s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 22/63 [03:41<06:34,  9.62s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/63 [03:52<06:33,  9.84s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 24/63 [04:04<06:53, 10.59s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 25/63 [04:13<06:20, 10.02s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 26/63 [04:23<06:10, 10.02s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 27/63 [04:33<06:04, 10.13s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 28/63 [04:42<05:36,  9.62s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 29/63 [04:52<05:34,  9.84s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/63 [05:02<05:29,  9.99s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 31/63 [05:11<05:04,  9.53s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 32/63 [05:21<05:03,  9.78s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 33/63 [05:31<04:57,  9.92s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 34/63 [05:40<04:35,  9.48s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 35/63 [05:50<04:33,  9.77s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 36/63 [06:04<04:52, 10.85s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 37/63 [06:13<04:31, 10.43s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 38/63 [06:23<04:20, 10.44s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 39/63 [06:33<04:02, 10.11s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 40/63 [06:42<03:47,  9.88s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 41/63 [06:53<03:40, 10.03s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 42/63 [07:01<03:22,  9.65s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 43/63 [07:11<03:14,  9.72s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 44/63 [07:22<03:08,  9.93s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 45/63 [07:30<02:51,  9.50s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 46/63 [07:42<02:55, 10.33s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 47/63 [07:53<02:45, 10.37s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 48/63 [08:02<02:31, 10.08s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 49/63 [08:12<02:17,  9.86s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 50/63 [08:22<02:10, 10.03s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 51/63 [08:31<01:56,  9.73s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 52/63 [08:41<01:46,  9.71s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 53/63 [08:51<01:39,  9.92s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 54/63 [09:00<01:25,  9.51s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 55/63 [09:10<01:17,  9.72s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 56/63 [09:20<01:09,  9.93s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 57/63 [09:29<00:56,  9.49s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 58/63 [09:39<00:48,  9.76s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 59/63 [09:50<00:39,  9.98s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 60/63 [09:58<00:28,  9.53s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 61/63 [10:09<00:19,  9.82s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 62/63 [10:19<00:09,  9.96s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 63/63 [10:24<00:00,  9.91s/it]\n",
            "Epoch:  20%|██        | 1/5 [10:24<41:36, 624.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss at Epoch 0: 0.07926585171963015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 1/63 [00:10<10:42, 10.37s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/63 [00:18<09:24,  9.25s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/63 [00:29<09:47,  9.78s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/63 [00:39<09:40,  9.83s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/63 [00:48<09:10,  9.49s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/63 [01:00<09:52, 10.39s/it]\u001b[A\n",
            "Iteration:  11%|█         | 7/63 [01:10<09:43, 10.42s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/63 [01:19<08:58,  9.80s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 9/63 [01:29<08:59,  9.99s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/63 [01:39<08:52, 10.05s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 11/63 [01:48<08:19,  9.61s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/63 [01:58<08:23,  9.87s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/63 [02:08<08:12,  9.85s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 14/63 [02:17<07:48,  9.57s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/63 [02:27<07:51,  9.82s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 16/63 [02:37<07:32,  9.63s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/63 [02:46<07:19,  9.56s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 18/63 [02:56<07:21,  9.81s/it]\u001b[A\n",
            "Iteration:  30%|███       | 19/63 [03:05<06:57,  9.50s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/63 [03:15<06:53,  9.62s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 21/63 [03:27<07:11, 10.27s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 22/63 [03:35<06:39,  9.76s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/63 [03:46<06:36,  9.92s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 24/63 [03:56<06:32, 10.06s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 25/63 [04:04<06:03,  9.55s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 26/63 [04:15<06:01,  9.78s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 27/63 [04:25<05:57,  9.93s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 28/63 [04:34<05:33,  9.52s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 29/63 [04:46<05:53, 10.39s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/63 [04:55<05:25,  9.87s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 31/63 [05:05<05:16,  9.90s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 32/63 [05:15<05:10, 10.03s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 33/63 [05:23<04:46,  9.56s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 34/63 [05:34<04:44,  9.80s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 35/63 [05:46<04:52, 10.44s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 36/63 [05:56<04:42, 10.45s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 37/63 [06:06<04:24, 10.16s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 38/63 [06:16<04:15, 10.23s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 39/63 [06:25<03:54,  9.75s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 40/63 [06:35<03:45,  9.81s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 41/63 [06:45<03:39,  9.99s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 42/63 [06:54<03:19,  9.52s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 43/63 [07:04<03:15,  9.76s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 44/63 [07:14<03:08,  9.92s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 45/63 [07:23<02:50,  9.47s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 46/63 [07:33<02:45,  9.74s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 47/63 [07:43<02:38,  9.89s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 48/63 [07:52<02:22,  9.47s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 49/63 [08:02<02:16,  9.74s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 50/63 [08:12<02:06,  9.74s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 51/63 [08:23<02:01, 10.10s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 52/63 [08:33<01:51, 10.16s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 53/63 [08:43<01:42, 10.21s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 54/63 [08:52<01:27,  9.69s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 55/63 [09:02<01:19,  9.89s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 56/63 [09:12<01:09,  9.94s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 57/63 [09:21<00:57,  9.57s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 58/63 [09:31<00:49,  9.81s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 59/63 [09:41<00:39,  9.77s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 60/63 [09:50<00:28,  9.56s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 61/63 [10:00<00:19,  9.81s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 62/63 [10:10<00:09,  9.59s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 63/63 [10:15<00:00,  9.77s/it]\n",
            "Epoch:  40%|████      | 2/5 [20:40<30:57, 619.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss at Epoch 1: 0.0042768856732263456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 1/63 [00:09<09:23,  9.09s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/63 [00:18<09:27,  9.31s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/63 [00:28<09:48,  9.80s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/63 [00:37<09:11,  9.34s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/63 [00:47<09:16,  9.59s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/63 [00:57<09:20,  9.84s/it]\u001b[A\n",
            "Iteration:  11%|█         | 7/63 [01:06<08:45,  9.38s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/63 [01:16<08:54,  9.72s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 9/63 [01:27<08:55,  9.92s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/63 [01:35<08:21,  9.46s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 11/63 [01:48<09:00, 10.39s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/63 [01:57<08:35, 10.11s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/63 [02:06<08:09,  9.80s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 14/63 [02:17<08:08,  9.97s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/63 [02:25<07:42,  9.64s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 16/63 [02:35<07:33,  9.65s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/63 [02:45<07:34,  9.88s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 18/63 [02:54<07:05,  9.45s/it]\u001b[A\n",
            "Iteration:  30%|███       | 19/63 [03:04<07:05,  9.66s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/63 [03:14<07:04,  9.88s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 21/63 [03:23<06:36,  9.45s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 22/63 [03:33<06:38,  9.72s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/63 [03:44<06:37,  9.93s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 24/63 [03:52<06:10,  9.50s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 25/63 [04:03<06:10,  9.76s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 26/63 [04:13<06:06,  9.91s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 27/63 [04:21<05:42,  9.50s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 28/63 [04:32<05:41,  9.76s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 29/63 [04:42<05:32,  9.78s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/63 [04:50<05:13,  9.51s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 31/63 [05:01<05:12,  9.77s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 32/63 [05:10<04:59,  9.65s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 33/63 [05:21<05:03, 10.11s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 34/63 [05:32<04:55, 10.19s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 35/63 [05:42<04:44, 10.17s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 36/63 [05:50<04:21,  9.69s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 37/63 [06:01<04:16,  9.88s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 38/63 [06:10<04:04,  9.79s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 39/63 [06:19<03:49,  9.56s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 40/63 [06:30<03:45,  9.79s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 41/63 [06:39<03:29,  9.51s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 42/63 [06:48<03:20,  9.56s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 43/63 [06:59<03:15,  9.79s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 44/63 [07:07<02:58,  9.39s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 45/63 [07:17<02:53,  9.66s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 46/63 [07:28<02:47,  9.87s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 47/63 [07:36<02:31,  9.44s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 48/63 [07:46<02:25,  9.71s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 49/63 [07:57<02:18,  9.88s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 50/63 [08:05<02:02,  9.44s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 51/63 [08:15<01:56,  9.70s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 52/63 [08:25<01:47,  9.74s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 53/63 [08:34<01:34,  9.46s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 54/63 [08:44<01:27,  9.73s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 55/63 [08:54<01:16,  9.59s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 56/63 [09:05<01:10, 10.12s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 57/63 [09:16<01:01, 10.26s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 58/63 [09:24<00:48,  9.71s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 59/63 [09:34<00:39,  9.90s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 60/63 [09:45<00:29,  9.97s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 61/63 [09:53<00:19,  9.53s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 62/63 [10:03<00:09,  9.77s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 63/63 [10:08<00:00,  9.66s/it]\n",
            "Epoch:  60%|██████    | 3/5 [30:48<20:28, 614.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss at Epoch 2: 0.0018442948441213323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 1/63 [00:10<10:37, 10.28s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/63 [00:19<10:06,  9.94s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/63 [00:28<09:28,  9.48s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/63 [00:39<09:39,  9.83s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/63 [00:48<09:15,  9.57s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/63 [00:57<09:02,  9.51s/it]\u001b[A\n",
            "Iteration:  11%|█         | 7/63 [01:08<09:08,  9.79s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/63 [01:16<08:38,  9.42s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 9/63 [01:26<08:38,  9.61s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/63 [01:37<08:41,  9.84s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 11/63 [01:45<08:09,  9.41s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/63 [01:55<08:13,  9.69s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/63 [02:06<08:14,  9.88s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 14/63 [02:14<07:42,  9.44s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/63 [02:26<08:14, 10.30s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 16/63 [02:37<08:04, 10.32s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/63 [02:45<07:30,  9.79s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 18/63 [02:55<07:23,  9.86s/it]\u001b[A\n",
            "Iteration:  30%|███       | 19/63 [03:06<07:20, 10.02s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/63 [03:14<06:50,  9.55s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 21/63 [03:25<06:51,  9.79s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 22/63 [03:35<06:48,  9.96s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/63 [03:43<06:19,  9.50s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 24/63 [03:54<06:20,  9.76s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 25/63 [04:04<06:16,  9.90s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 26/63 [04:12<05:50,  9.47s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 27/63 [04:23<05:50,  9.74s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 28/63 [04:32<05:40,  9.73s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 29/63 [04:41<05:23,  9.51s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/63 [04:52<05:22,  9.76s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 31/63 [05:01<05:05,  9.56s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 32/63 [05:10<04:56,  9.55s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 33/63 [05:21<04:54,  9.81s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 34/63 [05:29<04:34,  9.45s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 35/63 [05:39<04:29,  9.61s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 36/63 [05:50<04:25,  9.84s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 37/63 [05:59<04:09,  9.58s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 38/63 [06:11<04:15, 10.24s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 39/63 [06:20<04:01, 10.08s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 40/63 [06:29<03:44,  9.78s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 41/63 [06:40<03:39,  9.99s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 42/63 [06:49<03:25,  9.76s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 43/63 [06:59<03:13,  9.67s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 44/63 [07:09<03:08,  9.92s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 45/63 [07:18<02:53,  9.62s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 46/63 [07:28<02:44,  9.69s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 47/63 [07:38<02:38,  9.92s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 48/63 [07:47<02:22,  9.51s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 49/63 [07:57<02:16,  9.73s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 50/63 [08:08<02:09,  9.94s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 51/63 [08:16<01:53,  9.49s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 52/63 [08:26<01:47,  9.76s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 53/63 [08:37<01:39,  9.95s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 54/63 [08:45<01:25,  9.51s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 55/63 [08:56<01:18,  9.76s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 56/63 [09:06<01:09,  9.90s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 57/63 [09:14<00:57,  9.52s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 58/63 [09:25<00:48,  9.79s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 59/63 [09:35<00:40, 10.05s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 60/63 [09:46<00:30, 10.07s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 61/63 [09:56<00:20, 10.16s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 62/63 [10:04<00:09,  9.65s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 63/63 [10:11<00:00,  9.71s/it]\n",
            "Epoch:  80%|████████  | 4/5 [41:00<10:13, 613.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss at Epoch 3: 0.0010203922782758518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Iteration:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 1/63 [00:08<08:45,  8.47s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/63 [00:18<09:44,  9.58s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/63 [00:29<09:57,  9.96s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/63 [00:37<09:13,  9.38s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/63 [00:48<09:25,  9.76s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/63 [00:58<09:27,  9.96s/it]\u001b[A\n",
            "Iteration:  11%|█         | 7/63 [01:07<08:51,  9.49s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/63 [01:17<08:58,  9.79s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 9/63 [01:27<08:54,  9.89s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/63 [01:36<08:24,  9.52s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 11/63 [01:46<08:29,  9.79s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/63 [01:56<08:17,  9.75s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/63 [02:05<07:58,  9.57s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 14/63 [02:15<08:00,  9.81s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/63 [02:24<07:40,  9.60s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 16/63 [02:34<07:29,  9.57s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/63 [02:44<07:31,  9.81s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 18/63 [02:53<07:09,  9.54s/it]\u001b[A\n",
            "Iteration:  30%|███       | 19/63 [03:05<07:30, 10.25s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/63 [03:15<07:13, 10.09s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 21/63 [03:24<06:50,  9.78s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 22/63 [03:34<06:47,  9.94s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/63 [03:43<06:26,  9.67s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 24/63 [03:53<06:15,  9.64s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 25/63 [04:03<06:14,  9.85s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 26/63 [04:12<05:50,  9.48s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 27/63 [04:22<05:47,  9.65s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 28/63 [04:32<05:45,  9.87s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 29/63 [04:41<05:21,  9.46s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/63 [04:51<05:21,  9.75s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 31/63 [05:02<05:18,  9.96s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 32/63 [05:10<04:55,  9.52s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 33/63 [05:20<04:53,  9.78s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 34/63 [05:31<04:48,  9.94s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 35/63 [05:39<04:25,  9.50s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 36/63 [05:50<04:23,  9.75s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 37/63 [06:00<04:15,  9.84s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 38/63 [06:08<03:57,  9.51s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 39/63 [06:19<03:54,  9.76s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 40/63 [06:30<03:55, 10.25s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 41/63 [06:40<03:40, 10.02s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 42/63 [06:50<03:32, 10.13s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 43/63 [06:59<03:14,  9.73s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 44/63 [07:09<03:06,  9.81s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 45/63 [07:19<03:00, 10.01s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 46/63 [07:28<02:42,  9.56s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 47/63 [07:38<02:36,  9.80s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 48/63 [07:48<02:29,  9.97s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 49/63 [07:57<02:13,  9.51s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 50/63 [08:07<02:07,  9.78s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 51/63 [08:18<01:59,  9.97s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 52/63 [08:26<01:44,  9.50s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 53/63 [08:37<01:37,  9.76s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 54/63 [08:47<01:28,  9.86s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 55/63 [08:55<01:16,  9.52s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 56/63 [09:06<01:08,  9.79s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 57/63 [09:16<00:58,  9.79s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 58/63 [09:25<00:47,  9.58s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 59/63 [09:35<00:39,  9.81s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 60/63 [09:44<00:28,  9.64s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 61/63 [09:54<00:19,  9.59s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 62/63 [10:06<00:10, 10.45s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 63/63 [10:11<00:00,  9.70s/it]\n",
            "Epoch: 100%|██████████| 5/5 [51:11<00:00, 614.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss at Epoch 4: 0.0006582681856502498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's state_dict\n",
        "model.save_pretrained('bert_model')\n",
        "tokenizer.save_pretrained('bert_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ru3oBNhl9rI",
        "outputId": "c54e9879-fa77-4d37-c042-1d7f58d1401a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('bert_model/tokenizer_config.json',\n",
              " 'bert_model/special_tokens_map.json',\n",
              " 'bert_model/vocab.txt',\n",
              " 'bert_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r bert_model.zip bert_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx1eVEtb1J_i",
        "outputId": "92565091-e06e-4474-da86-46736b1141cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: bert_model/ (stored 0%)\n",
            "  adding: bert_model/vocab.txt (deflated 53%)\n",
            "  adding: bert_model/tokenizer_config.json (deflated 75%)\n",
            "  adding: bert_model/special_tokens_map.json (deflated 42%)\n",
            "  adding: bert_model/config.json (deflated 49%)\n",
            "  adding: bert_model/model.safetensors (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(toxicity_classifier, tokenizer, text):\n",
        "    # Prepare the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = toxicity_classifier(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    logits = outputs.logits.squeeze()  # Squeeze the logits to remove any extra dimensions\n",
        "\n",
        "    # Apply softmax if there are multiple classes (logits would be a 1D Tensor with more than one element)\n",
        "    if logits.numel() > 1:  # More than one element means multi-class classification\n",
        "        probabilities = torch.softmax(logits, dim=0)\n",
        "        predicted_class = torch.argmax(probabilities).item()\n",
        "        return probabilities, predicted_class\n",
        "    # Apply sigmoid if it's binary classification (logits would be a single-element tensor)\n",
        "    else:  # Single element means binary classification\n",
        "        probability = torch.sigmoid(logits).item()\n",
        "        return probability"
      ],
      "metadata": {
        "id": "3oOYIGMe1Xlz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"I like that shit.\"\n",
        "prediction = predict(model, tokenizer, example_text)\n",
        "\n",
        "# Process the prediction\n",
        "if isinstance(prediction, tuple):  # Multi-class classification\n",
        "    probabilities, predicted_class = prediction\n",
        "    print(f\"Probabilities: {probabilities[0].item()}\")\n",
        "    print(f\"Predicted toxic: {'Yes' if predicted_class == 0 else 'No'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPyYMqbb3l01",
        "outputId": "668e7c75-7c68-41e1-aad7-b44ffdf8645f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities: 0.9992675185203552\n",
            "Predicted toxic: Yes\n"
          ]
        }
      ]
    }
  ]
}